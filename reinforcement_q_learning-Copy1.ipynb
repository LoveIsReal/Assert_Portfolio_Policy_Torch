{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (DQN) Tutorial\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN algorithm\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2)\n",
    "    screen = screen[:, :, slice_range]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interaction with env one loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "for t in count():\n",
    "    action = select_action(state)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    \n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(len(memory))\n",
    "batch = Transition(*zip(*transitions))\n",
    "# 获取 next-state是否为 None\n",
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "# 对非 None的 next-state 筛选拼接\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "# 对state，action，reward进行拼接\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_final_mask: tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "non_final_next_state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "state: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "action: tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print('non_final_mask:', non_final_mask)\n",
    "print('non_final_next_state:', non_final_next_states[0][0][0])\n",
    "print('state:', state_batch[0][0])\n",
    "print('action:', action_batch)\n",
    "print('reward:', reward_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "# print('state_action_value:', state_action_values)\n",
    "# next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "# next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "# print('next_state_values:', next_state_values)\n",
    "#计算q的期望值\n",
    "# expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "# print('expected_state_action_values:', expected_state_action_values)\n",
    "#计算Huber loss\n",
    "# loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "# print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
